{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical foundations of neural nets\n",
    "# Show netral networks representing a 1d function f: R -> R\n",
    "# Train neural networks on samples of the functions -- regression\n",
    "# Start with linear networks only with linear activations\n",
    "# Then explore networks with relu activations and see how they fit \n",
    "# increasingly complicated functions\n",
    "#\n",
    "# Nicholas Miller (2023) In Jupyter Notebooks with Keras and Pyplot\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.initializers as init\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up samples for training and test sets\n",
    "# Training samples from the domain\n",
    "# Start label with simple linear funxtion y = 0.8 * x\n",
    "X_train = [[x/199. - 0.5] for x in range(200)]\n",
    "linear_Y_train = [[0.8 * x_i[0]] for x_i in X_train]\n",
    "\n",
    "X_test = [[x/199. + .0025 - 0.5] for x in range(0, 200, 10)]\n",
    "X_test_extra = [[x_i[0] + 0.4] for x_i in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a679319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the training data X, Y\n",
    "plt.plot(X_train, linear_Y_train, 'r-')\n",
    "plt.title('Training data x,y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously use a linear model to predict a linear function\n",
    "# Model is still linear no matter how many layers since all activations are linear\n",
    "linear_model = Sequential()\n",
    "linear_model.add(Dense(5, input_dim=1, activation='linear'))\n",
    "linear_model.add(Dense(1, activation='linear'))\n",
    "linear_model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the linear function with a linear model\n",
    "linear_model.fit(X_train, linear_Y_train, epochs=250, shuffle=True, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the fit worked by prediction the function on the test set\n",
    "linear_Y_predict = linear_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b65ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction (blue) vs actual function (red line)\n",
    "plt.plot(X_train, linear_Y_train, 'r-')\n",
    "plt.plot(X_test, linear_Y_predict, 'bo')\n",
    "plt.title('Linear model fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80191942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we try a linear model on a quadratic dataset -- the output will still be linear no matter the data\n",
    "# It underfits\n",
    "try_linear_model = Sequential()\n",
    "try_linear_model.add(Dense(5, input_dim=1, activation='linear'))\n",
    "try_linear_model.add(Dense(1, activation='linear'))\n",
    "try_linear_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "quadratic_Y_train = [[(x_i[0]-0.1)**2] for x_i in X_train]\n",
    "try_linear_model.fit(X_train, quadratic_Y_train, epochs=250, shuffle=True, verbose=0)\n",
    "try_quadratic = try_linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174285e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(X_train, quadratic_Y_train, 'r-')\n",
    "plt.plot(X_test, try_quadratic, 'bo')\n",
    "plt.title('Linear model to quadratic data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use relu activations\n",
    "# Obviously one node with one relu activation trivially learns relu\n",
    "relu_Y_train = [[x_i[0] if x_i[0] > 0 else 0.] for x_i in X_train]\n",
    "relu_model = Sequential()\n",
    "relu_model.add(Dense(1, input_dim=1, kernel_initializer=init.Constant(0.8), bias_initializer=init.Constant(0.1), activation='relu'))\n",
    "relu_model.add(Dense(1, activation='linear'))\n",
    "relu_model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=.2, momentum=.2))\n",
    "\n",
    "relu_model.fit(X_train, relu_Y_train, epochs=200, batch_size=600, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95644b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_predict = relu_model.predict(X_test)\n",
    "plt.plot(X_train, relu_Y_train, 'r-')\n",
    "plt.plot(X_test, relu_predict, 'bo')\n",
    "plt.title('RELU model to RELU data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "trelu_Y_train = [[1 - x_i[0] if x_i[0] < 0. else 1.] for x_i in X_train]\n",
    "trelu_model = Sequential()\n",
    "trelu_model.add(Dense(1, input_dim=1, kernel_initializer=init.Constant(-1.2), bias_initializer=init.Constant(0.03), activation='relu'))\n",
    "trelu_model.add(Dense(1, kernel_initializer=init.Constant(1.01), bias_initializer=init.Constant(0.99), activation='linear'))\n",
    "trelu_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "trelu_model.fit(X_train, trelu_Y_train, epochs=150, batch_size=400, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4db605",
   "metadata": {},
   "outputs": [],
   "source": [
    "trelu_predict = trelu_model.predict(X_test)\n",
    "plt.plot(X_train, trelu_Y_train, 'r-')\n",
    "plt.plot(X_test, trelu_predict, 'bo')\n",
    "plt.title('RELU model with trivially transformed RELU data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use multi-layered relu network to learn the quadratic we failed at before\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=1, activation='relu'))\n",
    "model.add(Dense(30, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, quadratic_Y_train, epochs=500, batch_size=200, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict based on the fitted quadratic\n",
    "quadratic = model.predict(X_test)\n",
    "plt.plot(X_train, quadratic_Y_train, 'r-')\n",
    "plt.plot(X_test, quadratic, 'bo')\n",
    "plt.title('Deep RELU model with quadratic data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well the function axtrapolates by predicint outside the bounds of the samples\n",
    "quadratic_extra = model.predict(X_test_extra)\n",
    "plt.plot(X_train, quadratic_Y_train, 'r-')\n",
    "plt.plot(X_test_extra, quadratic_extra, 'bo')\n",
    "plt.title('Deep RELU model extrapolating from quadratic data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try a bigger relu network and even more complex function -- sin(x)\n",
    "\n",
    "from math import sin\n",
    "from math import pi\n",
    "sin_Y_train = [[sin(x_i[0] * 4*pi) * 0.5 + 0.5] for x_i in X_train]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, sin_Y_train, epochs=750, batch_size=200, shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a model fitted to sin\n",
    "\n",
    "mysin = model.predict(X_test)\n",
    "plt.plot(X_train, sin_Y_train, 'r-')\n",
    "plt.plot(X_test, mysin, 'bo')\n",
    "plt.title(\"Deeper RELU model with sinusoidal data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4713d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3357e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how sin extrapolates outside of the sample domain\n",
    "sin_extra = model.predict(X_test_extra)\n",
    "plt.plot(X_train, sin_Y_train, 'r-')\n",
    "plt.plot(X_test_extra, sin_extra, 'bo')\n",
    "plt.title(\"Deeper RELU model extrapulating from sinusoidal data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16637b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
